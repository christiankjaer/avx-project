\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{listings}

\lstset{language=ML}

\title{Extending MLKit with vector instructions}
\author{Christian KjÃ¦r Larsen}



\begin{document}

\maketitle

\section{Introduction}

In this section we will briefly describe the project, its purpose and the structure of this report.

\subsection{Project statement}

The goal of this project is to add packed vector support to the MLKit\cite{mlkit} Standard ML compiler using the AVX2 vector instructions present in modern Intel processors. The motivation is to be able to use this support to optimize programs written in Standard ML to exploit data parallism.

\subsection{Roadmap}

\begin{enumerate}
    \item We start by investingating other approaches to SIMD programming in higher level languages. This is in order to settle on a
        good abstraction that is fairly easy to program with.
    \item
        We then continue by designing a programming abstraction to be able to optimize a program in a generic way that is not tied to a particular instruction set or vector extensions.
        We also write some simple example programs that will work as motivating examples and show that the abstraction is actually useful.
    \item
        Finally we implement compiler support for Intel AVX in the MLKit Standard ML compiler by providing a set of intrinsics that compile to efficient code that use native vector instructions.
\end{enumerate}

\section{Background}

\subsection{Vector instructions in modern CPUs}

MMX, SSE and AVX in Intel processors.

Neon on ARM processors.

\subsection{Programming model and higher level languages}

\subsubsection{Intrinsics}

Intrinsic functions for C/C++ guaranteeing what code is generated when the functions are called.

\subsubsection{Rust}

Standard interface to generic functions that are lowered directly to LLVM and compiled for specific architectures there.
\url{https://github.com/rust-lang/stdsimd}

\subsubsection{.NET and Java}

\url{https://docs.microsoft.com/en-us/dotnet/api/system.numerics.vector?view=net-5.0}


\subsubsection{Challenges}

Do not tie the implementations to tightly to the specific instruction set. It should be fairly easy to change a program from say AVX2 to Neon.

\section{Design}

MLKit has a 64 bit intel backend, and and we restrict ourselves to vectors of reals (64 bit floats) of 4 elements. This is supported by AVX2 which is widely supported now.

\section{Implementation}


\subsection{Generic interface}

\begin{lstlisting}[frame=single]
signature SIMD4 = sig
  type element = real
  type interface = real * real * real * real

  type simd
  type mask

  val mk : interface -> simd
  val read : simd -> interface

  (* arithmetic operations (vectors and scalars) *)
  val add : simd * simd -> simd
  val adds : simd * element -> simd
  (* and so on *)

  (* comparisons (vectors and scalars) *)
  val lt : simd * simd -> mask
  val lts : simd * element -> mask
  (* and so on *)

  (* predicates *)
  val all : mask -> boolean
  val any : mask -> boolean

  (* conditional operations *)
  val blend : simd * simd * mask -> simd
end
\end{lstlisting}


\subsubsection{Arithmetic}

Elementwise operations
Scalar operations

\subsubsection{Comparisons}

Mask type with all and any

\subsubsection{Masks}

Blend that 

\subsubsection{Handling arrays of odd length}


Basically a fast map on arrays with a mask at the end.
Tabulate would be nice.

Have like a RealTable but with SIMD operations. Odd lengths would be handled by blend.

\subsubsection{Vectorizing mandelbrot}

In the classic mandelbrot algorithm, a single pixel is calculated by applying a function to a coordinate until divergence.

\begin{lstlisting}
fun mandelbrot (px: int, py: int): int =
  let
    val x0 = scaleX px
    val y0 = scaleY py
    fun go iter x y =
      if (x*x + y*y <= 4.0 andalso iter < 1000)
      then go (iter + 1) (x*x - y*y + x0) (2.0*x*y + y0)
      else iter
  in
    go 0 0.0 0.0
  end
\end{lstlisting}

Using our structure we can compute 4 pixels at the same time using vector operations.

\begin{lstlisting}
functor Mandelbrot(Vec: SIMD4) = struct
  val zero = Vec.mk (0.0, 0.0, 0.0, 0.0)
  val one = Vec.mk (1.0, 1.0, 1.0, 1.0)

  fun mandelbrot (px: int, py: int): Vec.simd =
    let
      val x0: Vec.simd = scaleX px
      val y0: real = scaleY py
      fun go iter mask x =
        if (Vec.any mask andalso iter < 1000) then
          let
            val cmp = Vec.add (Vec.mul (x, x), Vec.mul (y, y))
            val newMask = Vec.lts (cmp, 4.0)
            val updated = Vec.blend (iters, Vec.add (iters, one), mask)
            val newX = Vec.blend (x, computeX, mask)
            val newY = Vec.blend (x, computeY, mask)
          in
            go (iter + 1) newMask updated newX newY
          end
        else iters
    in
      go 0 Vec.true zero zero zero
    end
end
\end{lstlisting}

We use the \verb!blend! function to conditionally update element only when the if-condition in the original program would have been true. It is not super high level, so it feels a bit like manually programming with low level vectors, but that is the point. We can use the module to optimize library functions for efficiency.




\subsection{Implementation in the MLKit}

Boxed representation that is available to the programmer and an internal unboxed type that is available to the optimizer.

\subsection{Data types}

Reuse register allocation of vector registers since they completely overlap with xmm. Just rename to ymm when they are used in a vector instruction.

\subsubsection{Primops}

Both boxed and unboxed version of intrinsics. The unboxed ones are not available the the programmer, but are used internally in the optimizer.

\subsubsection{Unboxing}

Since Standard ML is polymorphic, there might be polymorphic functions that can receive arguments of any type. This is typically achieved by having a uniform representation of values. Typically a box (pointer to the underlying data). There is a substantial penalty for this, since in order to do operations on the underlying value, there is a level of indirection. For our new vector type we will have to use 16 bytes of memory in addition to the pointer to represent something that typically fits in a register.

Any operation that works on a boxed representation will typically have to unbox the data, do the operation and box the result again. This is very costly on modern hardware.

In some sense
\[
    box (unbox\ x) \equiv x
\]
and
\[
    unbox (box\ x) \equiv x
\]

\bibliographystyle{unsrt}
\bibliography{simd}

\end{document}
